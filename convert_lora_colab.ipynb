{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”„ Convert LoRA to OpenVINO (Colab)\n",
    "\n",
    "Convert Vietnamese LoRA to OpenVINO format on Colab (free disk space!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q torch torchvision diffusers transformers accelerate safetensors openvino onnx\n!pip install -q --upgrade diffusers transformers"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¤ Step 2: Upload LoRA File\n",
    "\n",
    "Upload file `pytorch_lora_weights.safetensors` tá»« folder `lora_vietnamese/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create folder\n",
    "os.makedirs('lora_vietnamese', exist_ok=True)\n",
    "\n",
    "print(\"Upload file: pytorch_lora_weights.safetensors\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move to folder\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f'lora_vietnamese/{filename}')\n",
    "    print(f\"âœ“ Uploaded: lora_vietnamese/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 3: Conversion Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import openvino as ov\n",
    "from pathlib import Path\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import tempfile\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "def export_text_encoder(text_encoder, tokenizer, output_path):\n",
    "    \"\"\"Export CLIP text encoder\"\"\"\n",
    "    print(\"Converting Text Encoder...\")\n",
    "    \n",
    "    text_encoder.eval()\n",
    "    text_encoder = text_encoder.float()\n",
    "    \n",
    "    dummy_input = torch.ones((1, 77), dtype=torch.long)\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix=\".onnx\", delete=False) as tmp_file:\n",
    "        torch.onnx.export(\n",
    "            text_encoder,\n",
    "            dummy_input,\n",
    "            tmp_file.name,\n",
    "            input_names=[\"input_ids\"],\n",
    "            output_names=[\"hidden_states\"],\n",
    "            dynamic_axes={\n",
    "                \"input_ids\": {0: \"batch_size\"},\n",
    "                \"hidden_states\": {0: \"batch_size\"}\n",
    "            },\n",
    "            opset_version=14,\n",
    "            do_constant_folding=True\n",
    "        )\n",
    "        \n",
    "        ov_model = ov.convert_model(tmp_file.name)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        ov.save_model(ov_model, output_path / \"openvino_model.xml\")\n",
    "        \n",
    "        config = {\n",
    "            \"model_type\": \"CLIPTextModel\",\n",
    "            \"max_position_embeddings\": text_encoder.config.max_position_embeddings,\n",
    "            \"vocab_size\": text_encoder.config.vocab_size,\n",
    "            \"hidden_size\": text_encoder.config.hidden_size,\n",
    "        }\n",
    "        \n",
    "        with open(output_path / \"config.json\", 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ“ Text Encoder saved\")\n",
    "\n",
    "def export_unet(unet, output_path):\n",
    "    \"\"\"Export UNet\"\"\"\n",
    "    print(\"Converting UNet...\")\n",
    "    \n",
    "    unet.eval()\n",
    "    unet = unet.float()\n",
    "    \n",
    "    sample = torch.randn(1, 4, 64, 64, dtype=torch.float32)\n",
    "    timestep = torch.tensor([1], dtype=torch.long)\n",
    "    encoder_hidden_states = torch.randn(1, 77, 768, dtype=torch.float32)\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix=\".onnx\", delete=False) as tmp_file:\n",
    "        torch.onnx.export(\n",
    "            unet,\n",
    "            (sample, timestep, encoder_hidden_states),\n",
    "            tmp_file.name,\n",
    "            input_names=[\"sample\", \"timestep\", \"encoder_hidden_states\"],\n",
    "            output_names=[\"noise_pred\"],\n",
    "            dynamic_axes={\n",
    "                \"sample\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "                \"encoder_hidden_states\": {0: \"batch_size\"},\n",
    "                \"noise_pred\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}\n",
    "            },\n",
    "            opset_version=14,\n",
    "            do_constant_folding=True\n",
    "        )\n",
    "        \n",
    "        print(\"  Converting ONNX to OpenVINO IR...\")\n",
    "        ov_model = ov.convert_model(tmp_file.name)\n",
    "        \n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        ov.save_model(ov_model, output_path / \"openvino_model.xml\")\n",
    "        \n",
    "        config = unet.config\n",
    "        if hasattr(config, 'to_dict'):\n",
    "            config_dict = config.to_dict()\n",
    "        else:\n",
    "            config_dict = dict(config)\n",
    "            \n",
    "        with open(output_path / \"config.json\", 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ“ UNet saved\")\n",
    "\n",
    "def export_vae_encoder(vae, output_path):\n",
    "    \"\"\"Export VAE encoder\"\"\"\n",
    "    print(\"Converting VAE Encoder...\")\n",
    "    \n",
    "    vae.encoder.eval()\n",
    "    vae.encoder = vae.encoder.float()\n",
    "    \n",
    "    dummy_input = torch.randn(1, 3, 512, 512, dtype=torch.float32)\n",
    "    \n",
    "    class VAEEncoderWrapper(torch.nn.Module):\n",
    "        def __init__(self, encoder):\n",
    "            super().__init__()\n",
    "            self.encoder = encoder\n",
    "            \n",
    "        def forward(self, sample):\n",
    "            return self.encoder(sample)\n",
    "    \n",
    "    wrapper = VAEEncoderWrapper(vae.encoder)\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix=\".onnx\", delete=False) as tmp_file:\n",
    "        torch.onnx.export(\n",
    "            wrapper,\n",
    "            dummy_input,\n",
    "            tmp_file.name,\n",
    "            input_names=[\"sample\"],\n",
    "            output_names=[\"latent\"],\n",
    "            dynamic_axes={\n",
    "                \"sample\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "                \"latent\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}\n",
    "            },\n",
    "            opset_version=14,\n",
    "            do_constant_folding=True\n",
    "        )\n",
    "        \n",
    "        ov_model = ov.convert_model(tmp_file.name)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        ov.save_model(ov_model, output_path / \"openvino_model.xml\")\n",
    "        \n",
    "        config = {\n",
    "            \"model_type\": \"AutoencoderKL\",\n",
    "            \"in_channels\": vae.config.in_channels,\n",
    "            \"out_channels\": vae.config.out_channels,\n",
    "            \"latent_channels\": vae.config.latent_channels,\n",
    "            \"scaling_factor\": vae.config.scaling_factor,\n",
    "        }\n",
    "        \n",
    "        with open(output_path / \"config.json\", 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ“ VAE Encoder saved\")\n",
    "\n",
    "def export_vae_decoder(vae, output_path):\n",
    "    \"\"\"Export VAE decoder\"\"\"\n",
    "    print(\"Converting VAE Decoder...\")\n",
    "    \n",
    "    vae.decoder.eval()\n",
    "    vae.decoder = vae.decoder.float()\n",
    "    vae.post_quant_conv = vae.post_quant_conv.float()\n",
    "    \n",
    "    dummy_input = torch.randn(1, 4, 64, 64, dtype=torch.float32)\n",
    "    \n",
    "    class VAEDecoderWrapper(torch.nn.Module):\n",
    "        def __init__(self, decoder, post_quant_conv):\n",
    "            super().__init__()\n",
    "            self.decoder = decoder\n",
    "            self.post_quant_conv = post_quant_conv\n",
    "            \n",
    "        def forward(self, latent):\n",
    "            latent = self.post_quant_conv(latent)\n",
    "            return self.decoder(latent)\n",
    "    \n",
    "    wrapper = VAEDecoderWrapper(vae.decoder, vae.post_quant_conv)\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix=\".onnx\", delete=False) as tmp_file:\n",
    "        torch.onnx.export(\n",
    "            wrapper,\n",
    "            dummy_input,\n",
    "            tmp_file.name,\n",
    "            input_names=[\"latent\"],\n",
    "            output_names=[\"sample\"],\n",
    "            dynamic_axes={\n",
    "                \"latent\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "                \"sample\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}\n",
    "            },\n",
    "            opset_version=14,\n",
    "            do_constant_folding=True\n",
    "        )\n",
    "        \n",
    "        ov_model = ov.convert_model(tmp_file.name)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        ov.save_model(ov_model, output_path / \"openvino_model.xml\")\n",
    "        \n",
    "        config = {\n",
    "            \"model_type\": \"AutoencoderKL\",\n",
    "            \"in_channels\": vae.config.in_channels,\n",
    "            \"out_channels\": vae.config.out_channels,\n",
    "            \"latent_channels\": vae.config.latent_channels,\n",
    "            \"scaling_factor\": vae.config.scaling_factor,\n",
    "        }\n",
    "        \n",
    "        with open(output_path / \"config.json\", 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ“ VAE Decoder saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 4: Run Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Loading SD 1.5...\")\npipe = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    safety_checker=None\n)\npipe = pipe.to(\"cuda\")\n\nprint(\"Loading LoRA...\")\nlora_path = Path(\"lora_vietnamese/pytorch_lora_weights.safetensors\")\npipe.load_lora_weights(str(lora_path.parent), weight_name=lora_path.name)\n\nprint(\"Fusing LoRA...\")\npipe.fuse_lora(lora_scale=1.0)\n\nprint(\"Moving to CPU for export...\")\npipe = pipe.to(\"cpu\")\n\noutput_path = Path(\"vietnamese_ov\")\noutput_path.mkdir(exist_ok=True)\n\nprint(\"Converting Text Encoder...\")\nexport_text_encoder(pipe.text_encoder, pipe.tokenizer, output_path / \"text_encoder\")\n\nprint(\"Converting UNet...\")\nexport_unet(pipe.unet, output_path / \"unet\")\n\nprint(\"Converting VAE Encoder...\")\nexport_vae_encoder(pipe.vae, output_path / \"vae_encoder\")\n\nprint(\"Converting VAE Decoder...\")\nexport_vae_decoder(pipe.vae, output_path / \"vae_decoder\")\n\nprint(\"Saving additional files...\")\nwith tempfile.TemporaryDirectory() as tmp_dir:\n    pipe.save_pretrained(tmp_dir)\n    files_to_copy = [\n        \"tokenizer/merges.txt\",\n        \"tokenizer/vocab.json\",\n        \"tokenizer/special_tokens_map.json\",\n        \"tokenizer/tokenizer_config.json\",\n        \"scheduler/scheduler_config.json\",\n    ]\n    for file_path in files_to_copy:\n        src = Path(tmp_dir) / file_path\n        dst = output_path / file_path\n        if src.exists():\n            dst.parent.mkdir(parents=True, exist_ok=True)\n            shutil.copy2(src, dst)\n\nmodel_index = {\n    \"text_encoder\": [\"transformers\", \"CLIPTextModel\"],\n    \"tokenizer\": [\"transformers\", \"CLIPTokenizer\"],\n    \"unet\": [\"diffusers\", \"UNet2DConditionModel\"],\n    \"vae\": [\"diffusers\", \"AutoencoderKL\"],\n    \"scheduler\": [\"diffusers\", \"PNDMScheduler\"],\n    \"safety_checker\": None,\n    \"feature_extractor\": None,\n    \"_class_name\": \"StableDiffusionPipeline\",\n    \"_diffusers_version\": \"0.24.0\"\n}\nwith open(output_path / \"model_index.json\", 'w') as f:\n    json.dump(model_index, f, indent=2)\n\nprint(\"DONE\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 5: Zip vÃ  Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Creating zip file...\")\n",
    "shutil.make_archive('vietnamese_ov', 'zip', 'vietnamese_ov')\n",
    "print(\"âœ“ Zip created: vietnamese_ov.zip\")\n",
    "\n",
    "print(\"\\nDownloading...\")\n",
    "files.download('vietnamese_ov.zip')\n",
    "print(\"âœ“ Download started!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DONE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Extract vietnamese_ov.zip\")\n",
    "print(\"2. Copy folder 'vietnamese_ov' to D:/NCKH_OpenVINO/models/\")\n",
    "print(\"3. Run: python app_enhanced.py\")\n",
    "print(\"4. Select 'Vietnamese LoRA' in dropdown\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}