import os
import json
import shutil
from pathlib import Path
from PIL import Image, ImageEnhance, ImageFilter
import cv2
import numpy as np
from typing import List, Dict, Tuple, Optional
import argparse
from tqdm import tqdm
import hashlib
import urllib.request
from concurrent.futures import ThreadPoolExecutor, as_completed

class DatasetPreparator:
    """Class Ä‘á»ƒ chuáº©n bá»‹ vÃ  xá»­ lÃ½ dataset cho training"""
    
    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Thá»‘ng kÃª
        self.stats = {
            "total_images": 0,
            "processed_images": 0,
            "filtered_images": 0,
            "augmented_images": 0
        }
    
    def download_vietnamese_dataset(self) -> None:
        """Download dataset vá» Viá»‡t Nam tá»« cÃ¡c nguá»“n má»Ÿ"""
        
        print("ğŸ‡»ğŸ‡³ Downloading Vietnamese dataset...")
        
        # URLs cá»§a cÃ¡c dataset má»Ÿ vá» Viá»‡t Nam
        datasets = [
            {
                "name": "vietnam_landscapes",
                "urls": [
                    "https://example.com/vietnam_landscape_1.jpg",  # Thay báº±ng URLs tháº­t
                    # ThÃªm nhiá»u URLs khÃ¡c...
                ],
                "prompts": [
                    "beautiful Vietnamese landscape, mountains and valleys",
                    "traditional Vietnamese architecture",
                    "Vietnamese countryside scene"
                ]
            }
        ]
        
        # Táº¡o thÆ° má»¥c cho tá»«ng category
        for dataset in datasets:
            category_dir = self.output_dir / dataset["name"]
            category_dir.mkdir(exist_ok=True)
            
            # Download images
            for i, url in enumerate(tqdm(dataset["urls"], desc=f"Downloading {dataset['name']}")):
                try:
                    img_path = category_dir / f"img_{i:04d}.jpg"
                    urllib.request.urlretrieve(url, img_path)
                    
                    # Verify image
                    try:
                        img = Image.open(img_path)
                        img.verify()
                        self.stats["total_images"] += 1
                    except:
                        img_path.unlink()  # Delete corrupted image
                        
                except Exception as e:
                    print(f"Error downloading {url}: {e}")
    
    def create_vietnamese_prompts(self) -> List[str]:
        """Táº¡o danh sÃ¡ch prompts tiáº¿ng Viá»‡t vÃ  tiáº¿ng Anh"""
        
        vietnamese_prompts = [
            # Phong cáº£nh
            "phong cáº£nh Viá»‡t Nam Ä‘áº¹p, nÃºi non hÃ¹ng vÄ©",
            "vá»‹nh Háº¡ Long, Ä‘Ã¡ vÃ´i, thuyá»n buá»“m",
            "ruá»™ng lÃºa báº­c thang Sapa, mÃ u xanh",
            "phá»‘ cá»• Há»™i An, Ä‘Ã¨n lá»“ng Ä‘áº§y mÃ u sáº¯c",
            "sÃ´ng Mekong, Ä‘á»“ng báº±ng xanh tÆ°Æ¡i",
            
            # Kiáº¿n trÃºc
            "chÃ¹a Viá»‡t Nam cá»•, kiáº¿n trÃºc truyá»n thá»‘ng",
            "nhÃ  rÆ°á»ng xá»© Huáº¿, mÃ¡i ngÃ³i cong",
            "dinh thá»± PhÃ¡p cá»•, ÄÃ  Láº¡t",
            "lÄƒng táº©m hoÃ ng gia, Huáº¿",
            
            # Con ngÆ°á»i vÃ  vÄƒn hÃ³a
            "ngÆ°á»i phá»¥ ná»¯ Viá»‡t máº·c Ã¡o dÃ i truyá»n thá»‘ng",
            "nghá»‡ nhÃ¢n lÃ m gá»‘m BÃ¡t TrÃ ng",
            "mÃºa rá»‘i nÆ°á»›c truyá»n thá»‘ng",
            "lá»… há»™i Ä‘áº§u xuÃ¢n, mÃ u sáº¯c rá»±c rá»¡",
            
            # áº¨m thá»±c
            "phá»Ÿ Viá»‡t Nam, tÃ´ phá»Ÿ nÃ³ng há»•i",
            "bÃ¡nh mÃ¬ Viá»‡t Nam, giÃ²n tan",
            "chá»£ Viá»‡t Nam, hoa quáº£ tÆ°Æ¡i ngon",
            
            # ThiÃªn nhiÃªn
            "rá»«ng nguyÃªn sinh Viá»‡t Nam",
            "bÃ£i biá»ƒn Viá»‡t Nam, cÃ¡t tráº¯ng nÆ°á»›c trong",
            "hang Ä‘á»™ng Viá»‡t Nam, tháº¡ch nhÅ© ká»³ thÃº"
        ]
        
        # Translate to English prompts  
        english_prompts = [
            "beautiful Vietnamese landscape, majestic mountains",
            "Ha Long Bay, limestone karsts, sailing boats",
            "Sapa terraced rice fields, green color",
            "Hoi An ancient town, colorful lanterns",
            "Mekong River, fresh green delta",
            
            "Vietnamese ancient pagoda, traditional architecture", 
            "Hue traditional house, curved tile roof",
            "French colonial mansion, Da Lat",
            "imperial mausoleum, Hue",
            
            "Vietnamese woman wearing traditional ao dai",
            "Bat Trang pottery artisan",
            "traditional water puppet show",
            "New Year festival, vibrant colors",
            
            "Vietnamese pho, hot steaming bowl",
            "Vietnamese banh mi, crispy bread",
            "Vietnamese market, fresh fruits",
            
            "Vietnamese pristine forest",
            "Vietnamese beach, white sand clear water",
            "Vietnamese cave, stalactites and stalagmites"
        ]
        
        return vietnamese_prompts + english_prompts
    
    def filter_and_resize_images(self, source_dir: Path, min_resolution: int = 256, max_resolution: int = 1024) -> None:
        """Lá»c vÃ  resize áº£nh theo tiÃªu chuáº©n"""
        
        print(f"ğŸ” Filtering and resizing images from {source_dir}")
        
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        processed_dir = self.output_dir / "processed"
        processed_dir.mkdir(exist_ok=True)
        
        # Find all images
        image_paths = []
        for ext in image_extensions:
            image_paths.extend(source_dir.rglob(f"*{ext}"))
            image_paths.extend(source_dir.rglob(f"*{ext.upper()}"))
        
        for img_path in tqdm(image_paths, desc="Processing images"):
            try:
                # Load image
                img = Image.open(img_path).convert('RGB')
                w, h = img.size
                
                # Filter by resolution
                if min(w, h) < min_resolution or max(w, h) > max_resolution * 4:
                    self.stats["filtered_images"] += 1
                    continue
                
                # Filter by aspect ratio
                aspect_ratio = w / h
                if aspect_ratio > 3 or aspect_ratio < 1/3:
                    self.stats["filtered_images"] += 1
                    continue
                
                # Enhance image quality
                img = self.enhance_image_quality(img)
                
                # Smart resize
                img = self.smart_resize(img, target_size=512)
                
                # Generate unique filename
                img_hash = hashlib.md5(img.tobytes()).hexdigest()[:8]
                output_path = processed_dir / f"img_{img_hash}.jpg"
                
                # Save
                img.save(output_path, quality=95, optimize=True)
                self.stats["processed_images"] += 1
                
            except Exception as e:
                print(f"Error processing {img_path}: {e}")
                continue
    
    def enhance_image_quality(self, img: Image.Image) -> Image.Image:
        """TÄƒng cÆ°á»ng cháº¥t lÆ°á»£ng áº£nh"""
        
        # Remove noise
        img_array = np.array(img)
        img_array = cv2.bilateralFilter(img_array, 9, 75, 75)
        img = Image.fromarray(img_array)
        
        # Enhance sharpness
        enhancer = ImageEnhance.Sharpness(img)
        img = enhancer.enhance(1.1)
        
        # Enhance contrast
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(1.05)
        
        # Enhance color
        enhancer = ImageEnhance.Color(img)
        img = enhancer.enhance(1.1)
        
        return img
    
    def smart_resize(self, img: Image.Image, target_size: int = 512) -> Image.Image:
        """Resize thÃ´ng minh giá»¯ tá»· lá»‡"""
        
        w, h = img.size
        
        # Calculate resize ratio
        if w > h:
            new_w = target_size
            new_h = int(h * target_size / w)
        else:
            new_h = target_size  
            new_w = int(w * target_size / h)
        
        # Resize
        img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)
        
        # Pad to square
        canvas = Image.new('RGB', (target_size, target_size), (255, 255, 255))
        paste_x = (target_size - new_w) // 2
        paste_y = (target_size - new_h) // 2
        canvas.paste(img, (paste_x, paste_y))
        
        return canvas
    
    def augment_dataset(self, multiplier: int = 3) -> None:
        """TÄƒng cÆ°á»ng dataset báº±ng data augmentation"""
        
        print(f"ğŸ”„ Augmenting dataset with {multiplier}x multiplier")
        
        processed_dir = self.output_dir / "processed"
        augmented_dir = self.output_dir / "augmented" 
        augmented_dir.mkdir(exist_ok=True)
        
        # Copy original images
        for img_path in processed_dir.glob("*.jpg"):
            shutil.copy2(img_path, augmented_dir / img_path.name)
        
        # Generate augmented versions
        original_images = list(processed_dir.glob("*.jpg"))
        
        for img_path in tqdm(original_images, desc="Augmenting"):
            img = Image.open(img_path)
            base_name = img_path.stem
            
            for i in range(multiplier - 1):  # -1 vÃ¬ Ä‘Ã£ copy original
                # Random augmentation
                aug_img = self.apply_random_augmentation(img)
                
                # Save augmented image
                aug_path = augmented_dir / f"{base_name}_aug_{i+1}.jpg"
                aug_img.save(aug_path, quality=95)
                self.stats["augmented_images"] += 1
    
    def apply_random_augmentation(self, img: Image.Image) -> Image.Image:
        """Ãp dá»¥ng augmentation ngáº«u nhiÃªn"""
        
        import random
        
        # Random rotation (-15 to 15 degrees)
        if random.random() > 0.5:
            angle = random.uniform(-15, 15)
            img = img.rotate(angle, expand=False, fillcolor=(255, 255, 255))
        
        # Random brightness
        if random.random() > 0.5:
            enhancer = ImageEnhance.Brightness(img)
            factor = random.uniform(0.8, 1.2)
            img = enhancer.enhance(factor)
        
        # Random contrast
        if random.random() > 0.5:
            enhancer = ImageEnhance.Contrast(img)
            factor = random.uniform(0.8, 1.2)
            img = enhancer.enhance(factor)
        
        # Random saturation
        if random.random() > 0.5:
            enhancer = ImageEnhance.Color(img)
            factor = random.uniform(0.8, 1.2)
            img = enhancer.enhance(factor)
        
        # Random horizontal flip
        if random.random() > 0.5:
            img = img.transpose(Image.FLIP_LEFT_RIGHT)
        
        return img
    
    def create_metadata(self, use_vietnamese_prompts: bool = True) -> None:
        """Táº¡o metadata file vá»›i prompts"""
        
        print("ğŸ“ Creating metadata...")
        
        # Get all processed images
        final_dir = self.output_dir / "augmented"
        if not final_dir.exists():
            final_dir = self.output_dir / "processed"
        
        if not final_dir.exists():
            print("âŒ No processed images found!")
            return
        
        image_paths = list(final_dir.glob("*.jpg"))
        prompts = self.create_vietnamese_prompts() if use_vietnamese_prompts else []
        
        metadata = []
        
        for img_path in tqdm(image_paths, desc="Creating metadata"):
            # Random prompt selection
            import random
            if prompts:
                prompt = random.choice(prompts)
            else:
                # Generate generic prompt
                prompt = "high quality, detailed, photorealistic"
            
            metadata.append({
                "image": str(img_path.relative_to(final_dir)),
                "prompt": prompt,
                "caption": prompt,
                "width": 512,
                "height": 512
            })
        
        # Save metadata
        metadata_path = final_dir / "metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Metadata saved to {metadata_path}")
        print(f"ğŸ“Š Created metadata for {len(metadata)} images")
    
    def print_statistics(self) -> None:
        """In thá»‘ng kÃª dataset"""
        
        print("\nğŸ“Š Dataset Statistics:")
        print(f"  Total images found: {self.stats['total_images']}")
        print(f"  Successfully processed: {self.stats['processed_images']}")
        print(f"  Filtered out: {self.stats['filtered_images']}")
        print(f"  Augmented images: {self.stats['augmented_images']}")
        
        # Count final images
        final_dir = self.output_dir / "augmented"
        if not final_dir.exists():
            final_dir = self.output_dir / "processed"
        
        if final_dir.exists():
            final_count = len(list(final_dir.glob("*.jpg")))
            print(f"  Final dataset size: {final_count} images")

def main():
    parser = argparse.ArgumentParser(description="Prepare dataset for Stable Diffusion training")
    
    parser.add_argument("--source_dir", type=str, help="Source directory containing raw images")
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory for processed dataset")
    parser.add_argument("--download_vietnamese", action="store_true", help="Download Vietnamese dataset")
    parser.add_argument("--augment_multiplier", type=int, default=3, help="Data augmentation multiplier")
    parser.add_argument("--min_resolution", type=int, default=256, help="Minimum image resolution")
    parser.add_argument("--max_resolution", type=int, default=1024, help="Maximum image resolution")
    parser.add_argument("--use_vietnamese_prompts", action="store_true", help="Use Vietnamese prompts")
    
    args = parser.parse_args()
    
    # Initialize preparator
    preparator = DatasetPreparator(args.output_dir)
    
    # Download Vietnamese dataset if requested
    if args.download_vietnamese:
        preparator.download_vietnamese_dataset()
    
    # Process existing images
    if args.source_dir:
        source_path = Path(args.source_dir)
        if source_path.exists():
            preparator.filter_and_resize_images(
                source_path, 
                args.min_resolution, 
                args.max_resolution
            )
        else:
            print(f"âŒ Source directory {args.source_dir} does not exist!")
    
    # Augment dataset
    if args.augment_multiplier > 1:
        preparator.augment_dataset(args.augment_multiplier)
    
    # Create metadata
    preparator.create_metadata(args.use_vietnamese_prompts)
    
    # Print statistics
    preparator.print_statistics()
    
    print("âœ… Dataset preparation completed!")

if __name__ == "__main__":
    main() 