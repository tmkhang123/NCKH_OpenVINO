{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Training LoRA for Image-to-Image on Colab\n",
    "\n",
    "Notebook n√†y ƒë·ªÉ train LoRA tr√™n Google Colab v·ªõi GPU T4 mi·ªÖn ph√≠\n",
    "\n",
    "**Th·ªùi gian train:** 2-4 gi·ªù cho 15-30 ·∫£nh\n",
    "\n",
    "**GPU:** T4 (15GB VRAM) - FREE tier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã B∆∞·ªõc 1: Setup m√¥i tr∆∞·ªùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q diffusers transformers accelerate peft torch torchvision xformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ B∆∞·ªõc 2: Upload code v√† dataset\n",
    "\n",
    "**Option 1: Clone t·ª´ GitHub (khuy√™n d√πng)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository c·ªßa b·∫°n\n",
    "!git clone YOUR_REPO_URL\n",
    "%cd NCKH_OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Upload t·ª´ local**\n",
    "\n",
    "1. Zip folder training/ v√† dataset/\n",
    "2. Upload l√™n Colab:\n",
    "   - Click Files icon b√™n tr√°i\n",
    "   - Upload files\n",
    "3. Unzip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip uploaded files\n",
    "!unzip -q training.zip\n",
    "!unzip -q dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3: Download dataset t·ª´ Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy dataset t·ª´ Drive\n",
    "!cp -r \"/content/drive/MyDrive/your_dataset\" ./dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä B∆∞·ªõc 3: Chu·∫©n b·ªã dataset\n",
    "\n",
    "Dataset structure c·∫ßn:\n",
    "```\n",
    "dataset/\n",
    "‚îú‚îÄ‚îÄ image1.jpg\n",
    "‚îú‚îÄ‚îÄ image2.jpg\n",
    "‚îú‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ metadata.json  # T·ª± ƒë·ªông t·∫°o n·∫øu ch∆∞a c√≥\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(\"./dataset\")\n",
    "image_files = list(dataset_path.glob(\"*.jpg\")) + list(dataset_path.glob(\"*.png\"))\n",
    "\n",
    "print(f\"Found {len(image_files)} images\")\n",
    "print(\"\\nFirst 5 images:\")\n",
    "for img in image_files[:5]:\n",
    "    print(f\"  - {img.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset with prompts\n",
    "!python training/dataset_preparation.py \\\n",
    "    --source_dir ./dataset \\\n",
    "    --output_dir ./processed_dataset \\\n",
    "    --augment_multiplier 3 \\\n",
    "    --use_vietnamese_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì B∆∞·ªõc 4: Train LoRA\n",
    "\n",
    "**Tham s·ªë quan tr·ªçng:**\n",
    "- `--rank`: 4-16 (rank c√†ng cao = model c√†ng m·∫°nh nh∆∞ng file c√†ng l·ªõn)\n",
    "- `--learning_rate`: 1e-4 ƒë·∫øn 5e-5\n",
    "- `--num_train_epochs`: 10-30 epochs\n",
    "- `--train_batch_size`: 1-2 (t√πy VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "!python training/train_lora.py \\\n",
    "    --data_dir ./processed_dataset/augmented \\\n",
    "    --output_dir ./lora_output \\\n",
    "    --pretrained_model_name_or_path \"runwayml/stable-diffusion-v1-5\" \\\n",
    "    --resolution 512 \\\n",
    "    --train_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --num_train_epochs 20 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --rank 8 \\\n",
    "    --alpha 32 \\\n",
    "    --mixed_precision \"fp16\" \\\n",
    "    --validation_prompt \"beautiful Vietnamese landscape, mountains\" \\\n",
    "    --validation_steps 100 \\\n",
    "    --save_steps 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç B∆∞·ªõc 5: Test LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trained LoRA\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load base model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Load LoRA\n",
    "pipe.unet.load_attn_procs(\"./lora_output\")\n",
    "\n",
    "# Test generation\n",
    "prompt = \"beautiful Vietnamese landscape, rice terraces, mountains\"\n",
    "image = pipe(prompt, num_inference_steps=30).images[0]\n",
    "image.save(\"test_result.png\")\n",
    "\n",
    "# Display\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ B∆∞·ªõc 6: Convert to OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenVINO\n",
    "!pip install -q openvino openvino-dev\n",
    "\n",
    "# Convert LoRA-merged model to OpenVINO\n",
    "!python training/convert_to_openvino.py \\\n",
    "    --model_path \"runwayml/stable-diffusion-v1-5\" \\\n",
    "    --lora_path ./lora_output \\\n",
    "    --output_path ./model_with_lora_ov \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• B∆∞·ªõc 7: Download k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip LoRA weights\n",
    "!zip -r lora_weights.zip ./lora_output\n",
    "!zip -r openvino_model.zip ./model_with_lora_ov\n",
    "\n",
    "# Download v·ªÅ local\n",
    "from google.colab import files\n",
    "files.download('lora_weights.zip')\n",
    "files.download('openvino_model.zip')\n",
    "\n",
    "# Ho·∫∑c copy v√†o Drive\n",
    "!cp lora_weights.zip \"/content/drive/MyDrive/\"\n",
    "!cp openvino_model.zip \"/content/drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Training Tips\n",
    "\n",
    "### **N·∫øu g·∫∑p OOM (Out of Memory):**\n",
    "```python\n",
    "# Gi·∫£m batch size\n",
    "--train_batch_size 1\n",
    "\n",
    "# TƒÉng gradient accumulation\n",
    "--gradient_accumulation_steps 8\n",
    "\n",
    "# Gi·∫£m resolution\n",
    "--resolution 384\n",
    "```\n",
    "\n",
    "### **ƒê·ªÉ train nhanh h∆°n:**\n",
    "```python\n",
    "# Enable xformers\n",
    "!pip install xformers\n",
    "\n",
    "# Gi·∫£m validation frequency\n",
    "--validation_steps 500\n",
    "\n",
    "# Gi·∫£m epochs n·∫øu dataset l·ªõn\n",
    "--num_train_epochs 10\n",
    "```\n",
    "\n",
    "### **ƒê·ªÉ quality t·ªët h∆°n:**\n",
    "```python\n",
    "# TƒÉng rank (nh∆∞ng file l·ªõn h∆°n)\n",
    "--rank 16\n",
    "\n",
    "# TƒÉng epochs\n",
    "--num_train_epochs 30\n",
    "\n",
    "# Lower learning rate\n",
    "--learning_rate 5e-5\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
