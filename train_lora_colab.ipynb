{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LoRA on Colab\n",
    "\n",
    "Dataset already on Drive: style_1_vietnamese (40 images)\n",
    "\n",
    "Training time: 45-60 minutes on T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate peft torch torchvision datasets\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "dataset_path = \"/content/drive/MyDrive/NCKH_Datasets/style_1_vietnamese\"\n",
    "output_path = \"/content/drive/MyDrive/NCKH_LoRAs/lora_vietnamese\"\n",
    "\n",
    "# Training params\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "RANK = 8\n",
    "MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "print(f\"Dataset: {dataset_path}\")\n",
    "print(f\"Output: {output_path}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"LoRA Rank: {RANK}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Count images\n",
    "images = list(Path(dataset_path).glob(\"*.jpg\")) + list(Path(dataset_path).glob(\"*.png\"))\n",
    "print(f\"Found {len(images)} images\")\n",
    "\n",
    "if len(images) < 10:\n",
    "    print(\"ERROR: Not enough images!\")\n",
    "else:\n",
    "    print(\"Dataset OK\")\n",
    "    \n",
    "# Create metadata if not exists\n",
    "metadata_file = Path(dataset_path) / \"metadata.json\"\n",
    "\n",
    "if not metadata_file.exists():\n",
    "    print(\"Creating metadata...\")\n",
    "    metadata = []\n",
    "    for img_path in images:\n",
    "        metadata.append({\n",
    "            \"image\": img_path.name,\n",
    "            \"prompt\": \"beautiful vietnamese landscape, natural scenery, high quality\",\n",
    "            \"caption\": \"vietnamese landscape photography\"\n",
    "        })\n",
    "    \n",
    "    with open(metadata_file, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(\"Metadata created\")\n",
    "else:\n",
    "    print(\"Metadata already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "print(\"Loading models...\")\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(MODEL_ID, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(MODEL_ID, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(MODEL_ID, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(MODEL_ID, subfolder=\"unet\")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
    "\n",
    "# Freeze\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "print(\"Models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=RANK,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "    lora_dropout=0.0,\n",
    ")\n",
    "\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.print_trainable_parameters()\n",
    "\n",
    "print(\"LoRA configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, tokenizer):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        with open(self.data_dir / \"metadata.json\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(512),\n",
    "            transforms.CenterCrop(512),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.metadata[idx]\n",
    "        img = Image.open(self.data_dir / item[\"image\"]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        input_ids = self.tokenizer(\n",
    "            item[\"prompt\"],\n",
    "            max_length=77,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        return {\"pixel_values\": img, \"input_ids\": input_ids}\n",
    "\n",
    "dataset = ImageDataset(dataset_path, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Dataset ready: {len(dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "unet, optimizer, dataloader = accelerator.prepare(unet, optimizer, dataloader)\n",
    "vae.to(accelerator.device)\n",
    "text_encoder.to(accelerator.device)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total steps: {EPOCHS * len(dataloader)}\")\n",
    "\n",
    "unet.train()\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        with accelerator.accumulate(unet):\n",
    "            # Encode to latent\n",
    "            latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n",
    "            latents = latents * 0.18215\n",
    "            \n",
    "            # Noise\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, 1000, (latents.shape[0],), device=latents.device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            # Text embeddings\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "            \n",
    "            # Predict noise\n",
    "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            \n",
    "            # Loss\n",
    "            loss = F.mse_loss(model_pred.float(), noise.float())\n",
    "            \n",
    "            # Backward\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step % 50 == 0:\n",
    "            print(f\"Step {global_step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p \"{output_path}\"\n",
    "\n",
    "# Unwrap and save\n",
    "unet_lora = accelerator.unwrap_model(unet)\n",
    "unet_lora.save_pretrained(output_path)\n",
    "\n",
    "print(f\"LoRA saved to: {output_path}\")\n",
    "\n",
    "# List saved files\n",
    "!ls -lh \"{output_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Test LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "print(\"Testing LoRA...\")\n",
    "\n",
    "# Load pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Load LoRA\n",
    "pipe.unet.load_attn_procs(output_path)\n",
    "\n",
    "# Test generation\n",
    "prompt = \"beautiful vietnamese landscape with mountains and rice fields\"\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=30,\n",
    "    guidance_scale=7.5\n",
    ").images[0]\n",
    "\n",
    "image.save(\"/content/test_lora.png\")\n",
    "print(\"Test image saved\")\n",
    "\n",
    "# Display\n",
    "from IPython.display import display\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Download LoRA to Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip LoRA weights\n",
    "!cd \"{output_path}\" && zip -r /content/lora_vietnamese.zip .\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('/content/lora_vietnamese.zip')\n",
    "\n",
    "print(\"Download started!\")\n",
    "print(\"File: lora_vietnamese.zip (20-50MB)\")\n",
    "print(\"Extract and copy to: models/lora/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "After download:\n",
    "1. Extract lora_vietnamese.zip\n",
    "2. Copy pytorch_lora_weights.safetensors to models/lora/vietnamese.safetensors\n",
    "3. Test in local app\n",
    "\n",
    "LoRA is saved on Drive at: /content/drive/MyDrive/NCKH_LoRAs/lora_vietnamese/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
